---
title: "Introduction to PyTorch"
subtitle: 
author: "Riley Harper"
institute: "UNC-Chapel Hill"
output:
  xaringan::moon_reader:
    lib_dir: libs
    mathjax: default
    css: PytorchLecture.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
---
```{r include=FALSE}
packages <- c("chromote", "pdftools", "xaringanExtra", "haven", "readr", "dplyr", "knitr", "withr", "htmltools", "reticulate", "xaringan", "kableExtra")

for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package)
    library(package, character.only = TRUE)
  }
}

xaringanExtra::use_tile_view()
xaringanExtra::use_share_again()
xaringanExtra::use_broadcast()
xaringanExtra::use_scribble(pen_color = "#a4c4f4", pen_size = 8, eraser_size = 30)
xaringanExtra::use_fit_screen()
```

class: center, middle

# **What is <img src="./media/pytorch_logo.png" alt = "Pytorch Logo" height="60px" style="vertical-align: middle; filter: contrast(1.3) brightness(0.9);"/> ?**

### An open-source deep learning (and ML) library for Python, primarily developed by Facebook's AI Research (FAIR) team.

###### *with major contributions from Adam Paszke, Soumith Chintala, Sam Gross, Gregory Chanan, and Zeming Lin.

---
class: middle

# Why PyTorch?
- Easy to learn and use
- Dynamic computation graph (define-by-run)
- Strong community and industry adoption
- Extensive support for deep learning tasks

---
class: center, middle

### PyTorch vs Competitors

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# First half of the comparison table
comparison_table_part1 <- data.frame(
   Feature = c("Dynamic computation graph", "Ease of use", "Deployment options", 
              "Community support"),
  PyTorch = c("Yes (define-by-run)", "High (Pythonic)", "Moderate", 
              "Strong"),
  TensorFlow = c("No (Static by default, eager execution available)", "Moderate", 
                 "High", "Strong"),
  Keras = c("No (Runs on TensorFlow)", "High", "High", 
            "Strong"),
  JAX = c("Yes", "Moderate", "Moderate", 
          "Growing")
)

# Use kable with column header customization
kable(comparison_table_part1, format = "html", escape = FALSE, col.names = c(
  cell_spec("", extra_css = "visibility: hidden;"), "PyTorch", "TensorFlow", "Keras", "JAX"
)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = TRUE) %>%
  column_spec(1, bold = TRUE, background = "#f2f2f2")  %>%  # Style the first column
  column_spec(2:5, extra_css = "text-align: center;") %>%  # Center-align columns 2 to 5 content
  row_spec(0, align = "center")  # Center-align headers
```

---
class: center, middle

### PyTorch vs Competitors (cont.)
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Second half of the comparison table
comparison_table_part2 <- data.frame(
  Feature = c("Popular for research", "GPU support", "Model interpretability"),
  PyTorch = c("Very popular", "Excellent (via CUDA)", "Good"),
  TensorFlow = c("Popular", "Excellent", "Moderate"),
  Keras = c("Less popular", "Via TensorFlow", "Moderate"),
  JAX = c("Increasing", "Excellent", "Moderate")
)

# Render the table with centered headers and content for columns 2 to 5
kable(comparison_table_part2, format = "html", escape = FALSE, col.names = c(
  cell_spec("", extra_css = "visibility: hidden;"), "PyTorch", "TensorFlow", "Keras", "JAX"
)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = TRUE) %>%
  column_spec(1, bold = TRUE, background = "#f2f2f2") %>%  # Style the first column
  column_spec(2:5, extra_css = "text-align: center;") %>%  # Center-align columns 2 to 5 content
  row_spec(0, align = "center")  # Center-align headers
```

---
class: middle
# Tensors
### PyTorch's basic data structure, similar to NumPy arrays but with the following additional features,
- **GPU acceleration**, for faster computations.
- **Autograd support**, enabling automatic differentiation for neural network training.
- **Multi-dimensionality**, ranging from scalars to high-dimensional arrays for handling complex data like images and time series.

---
class: middle
# Tensors Example
```{python}
import torch
x = torch.tensor([1.0, 2.0, 3.0])
print(x)
```

---
class: middle
# <div style="font-size: 0.8em">Autograd (Automatic Differentiation) </div>
### PyTorch's mechanism for automatic differentiation, useful for:
- **Tracking gradients**: Allows tensors to remember operations for backpropagation.
- **Enabling gradient-based optimization**: Useful for training neural networks by adjusting weights using gradients.
- **Ease of use**: Automatically calculates gradients without needing to manually compute derivatives.

---
class: middle
# Autograd Example
```{python, echo=TRUE, include=TRUE, results='hide'}
import torch

# Creating a tensor with requires_grad=True to track gradients
x = torch.tensor(3.0, requires_grad=True)
y = x**2
y.backward()  # Perform backpropagation
print(x.grad)  # Print the gradient of y with respect to x
```

---
class: middle
### **Backpropagation**

- Fundamental algorithm in deep learning used to train neural networks. 
- Computes the gradient of the loss function with respect to each weight in the network, allowing the model to learn by updating these weights using gradient descent.
- Applies the chain rule to compute gradients layer by layer, enabling efficient gradient calculation for large networks
- PyTorchâ€™s **Autograd** simplifies this by automatically tracking operations on tensors and computing gradients, making backpropagation more manageable for complex models.

---
class: middle

### **Backpropagation Example**

#### Given the function \( y = x^2 \), let \( x = 3.0 \) and compute the gradient of \( y \) with respect to \( x \) using PyTorch's **Autograd**.

#### Backpropagation steps:
<ol>
<li style="font-size: 1em;"><b>Define the function</b>: \( y = x^2 \)</li>
<li style="font-size: 1em;"><b>Compute the gradient</b>: The derivative of \( y \) with respect to \( x \) is \( \frac{dy}{dx} = 2x \)</li>
<li style="font-size: 1em;"><b>Evaluate the gradient at \( x = 3.0 \)</b>: \( \frac{dy}{dx} = 2(3.0) = 6.0 \)</li>
<li style="font-size: 1em;"><b>PyTorch computes this gradient automatically</b> and stores it in  \( x.grad \).</li>
</ol>

#### Result:
<ul>
<li style="font-size: 1em;">The computed gradient is: \( \frac{dy}{dx} = 6.0 \)</li>
</ul>

---
class: middle
# Autograd Example w Result
```{python}
import torch

# Creating a tensor with requires_grad=True to track gradients
x = torch.tensor(3.0, requires_grad=True)
y = x**2
y.backward()  # Perform backpropagation
print(x.grad)  # Print the gradient of y with respect to x
```

---
class: middle
<style>
.ANN-text {
  font-weight: normal;
}
</style>

#### <b>Artificial Neural Networks</b>

##### <span class="ANN-text">Artificial Neural Networks (ANNs) are computational models inspired by the human brain, consisting of layers of interconnected neurons. Each neuron receives input, processes it using a weighted sum and an activation function, and passes the result to the next layer. ANNs are widely used in deep learning for tasks like image recognition, natural language processing, and more [<a href="https://link.springer.com/article/10.1007/BF02478259" target="_blank">McCulloch & Pitts, 1943</a>][<a href="https://psycnet.apa.org/doi/10.1037/h0042519" target="_blank">Rosenblatt, 1958</a>][<a href="https://www.nature.com/articles/323533a0" target="_blank">Rumelhart, Hinton, & Williams, 1986</a>][<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" target="_blank">LeCun et al., 1998</a>].</span>

<ul>
<li style="font-size: 0.8em;"><b>Input Layer</b>: Receives the input data. 
    <br> 
    <i>Example</i>: For fMRI image analysis, the input layer might take voxel intensity values from a 3D fMRI scan, where each voxel represents brain activity at a given point in space.</li>

<li style="font-size: 0.8em;"><b>Hidden Layers</b>: Perform computations and transformations on the data. 
    <br> 
    <i>Example</i>: In fMRI data analysis, hidden layers could detect patterns or regions of interest related to brain activity, such as identifying active regions during a specific cognitive task.</li>

<li style="font-size: 0.8em;"><b>Output Layer</b>: Produces the final result based on the previous layers. 
    <br> 
    <i>Example</i>: The output layer in an fMRI model might classify brain activity as corresponding to different cognitive states, such as distinguishing between a resting state and task-induced activity.</li>

<li style="font-size: 0.8em;"><b>Weights and Biases</b>: Parameters that the model learns to minimize the error. 
    <br> 
    <i>Example</i>: During training, the model adjusts the weights and biases to correctly predict patterns in brain activity, such as recognizing specific neural signatures linked to diseases like Alzheimer's.</li>

<li style="font-size: 0.8em;"><b>Activation Function</b>: Introduces non-linearity to allow learning complex patterns. 
    <br> 
    <i>Example</i>: The ReLU (Rectified Linear Unit) activation function could be used in an fMRI model to detect complex non-linear relationships between brain regions, ensuring the model captures intricate patterns of brain activity.</li>
</ul>

---
class: middle
# Building a Neural Network

```{python}
import torch.nn as nn

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.fc(x)

model = Net()
print(model)
```

---
class: middle
# Training Loop

```{python}
import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Dummy data
inputs = torch.randn(10)
targets = torch.randn(1)

# Training step
optimizer.zero_grad()
output = model(inputs)
loss = criterion(output, targets)
loss.backward()
optimizer.step()

print('Loss:', loss.item())
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)

```{python, echo=TRUE, eval=FALSE, include=TRUE, results='hide'}
import torch
import torch.nn as nn
import nilearn
from nilearn.datasets import fetch_neurovault_motor_task
from nilearn.input_data import NiftiMasker

# Load fMRI data from the NeuroVault motor task dataset
# NeuroVault is a repository for brain imaging data, and the motor task dataset contains fMRI images from participants performing motor tasks
dataset = fetch_neurovault_motor_task()

# The dataset contains multiple fMRI images, we are selecting the first image from the dataset
fmri_img = dataset.images[0]

# Masking: Transform the 3D fMRI image into a 2D array where each voxel is treated as a feature
# NiftiMasker standardizes the data (voxel intensities) and prepares it for machine learning
# This will flatten the 3D image into a 2D array with shape (number of scans, number of voxels)
masker = NiftiMasker(standardize=True)
fmri_data = masker.fit_transform(fmri_img)
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)

```{python, echo=TRUE, eval=FALSE, include=TRUE, results='hide'}
# Define a simple neural network for fMRI data analysis
class fMRI_Net(nn.Module):
    def __init__(self):
        super(fMRI_Net, self).__init__()
        # The first fully connected layer takes the number of voxels as input (e.g., 1000+ voxels)
        # and outputs 128 features. The number of input features is dynamically set by fmri_data.shape[1]
        self.fc1 = nn.Linear(fmri_data.shape[1], 128)  # Input to hidden layer
        
        # The second fully connected layer reduces 128 features to 64 features
        self.fc2 = nn.Linear(128, 64)  # Hidden layer
        
        # The output layer has 2 neurons for binary classification (e.g., motor task vs. rest)
        self.fc3 = nn.Linear(64, 2)  # Output layer
        
        # ReLU activation function for non-linearity, used after the hidden layers
        self.relu = nn.ReLU()
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)

```{python, echo=TRUE, eval=FALSE, include=TRUE, results='hide'}
    # Define the forward pass of the network
    def forward(self, x):
        x = self.relu(self.fc1(x))  # Apply first layer and ReLU activation
        x = self.relu(self.fc2(x))  # Apply second layer and ReLU activation
        x = self.fc3(x)  # Output layer (no activation function needed for the output)
        return x

# Create a PyTorch tensor from the first scan's voxel data (fmri_data[0])
# This converts the fMRI data into a tensor of floats to be used in the network
fMRI_input = torch.tensor(fmri_data[0], dtype=torch.float32)

# Initialize the neural network
model = fMRI_Net()

# Perform a forward pass through the network with the fMRI input data
output = model(fMRI_input)
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)

```{python, echo=FALSE, eval=TRUE, include=TRUE, results='hide'}
import torch
import torch.nn as nn
import nilearn
from nilearn.datasets import fetch_neurovault_motor_task
from nilearn.input_data import NiftiMasker

# Load fMRI data from the NeuroVault motor task dataset
# NeuroVault is a repository for brain imaging data, and the motor task dataset contains fMRI images from participants performing motor tasks
dataset = fetch_neurovault_motor_task()

# The dataset contains multiple fMRI images, we are selecting the first image from the dataset
fmri_img = dataset.images[0]

# Masking: Transform the 3D fMRI image into a 2D array where each voxel is treated as a feature
# NiftiMasker standardizes the data (voxel intensities) and prepares it for machine learning
# This will flatten the 3D image into a 2D array with shape (number of scans, number of voxels)
masker = NiftiMasker(standardize=True)
fmri_data = masker.fit_transform(fmri_img)

# Define a simple neural network for fMRI data analysis
class fMRI_Net(nn.Module):
    def __init__(self):
        super(fMRI_Net, self).__init__()
        # The first fully connected layer takes the number of voxels as input (e.g., 1000+ voxels)
        # and outputs 128 features. The number of input features is dynamically set by fmri_data.shape[1]
        self.fc1 = nn.Linear(fmri_data.shape[1], 128)  # Input to hidden layer
        
        # The second fully connected layer reduces 128 features to 64 features
        self.fc2 = nn.Linear(128, 64)  # Hidden layer
        
        # The output layer has 2 neurons for binary classification (e.g., motor task vs. rest)
        self.fc3 = nn.Linear(64, 2)  # Output layer
        
        # ReLU activation function for non-linearity, used after the hidden layers
        self.relu = nn.ReLU()
        
    # Define the forward pass of the network
    def forward(self, x):
        x = self.relu(self.fc1(x))  # Apply first layer and ReLU activation
        x = self.relu(self.fc2(x))  # Apply second layer and ReLU activation
        x = self.fc3(x)  # Output layer (no activation function needed for the output)
        return x

# Create a PyTorch tensor from the first scan's voxel data (fmri_data[0])
# This converts the fMRI data into a tensor of floats to be used in the network
fMRI_input = torch.tensor(fmri_data[0], dtype=torch.float32)

# Initialize the neural network
model = fMRI_Net()

# Perform a forward pass through the network with the fMRI input data
output = model(fMRI_input)
```

```{python, echo=TRUE, eval=TRUE, include=TRUE}
# Print the architecture of the model (layers and their sizes)
print(model)

# Print the output of the model
# This will be a tensor with 2 values representing the output from the 2 neurons in the final layer
# These 2 values can be interpreted as the scores for the 2 classes (e.g., motor task vs. rest)
print(output)
```
---
class: center, middle
# Questions?