<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to PyTorch</title>
    <meta charset="utf-8" />
    <meta name="author" content="Riley Harper" />
    <script src="libs/header-attrs-2.28/header-attrs.js"></script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="libs/shareon-1.4.1/shareon.min.css" rel="stylesheet" />
    <script src="libs/shareon-1.4.1/shareon.min.js"></script>
    <link href="libs/xaringanExtra-shareagain-0.2.6/shareagain.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-shareagain-0.2.6/shareagain.js"></script>
    <script src="libs/js-cookie-3.0.0/js.cookie.js"></script>
    <script src="libs/peerjs-1.3.1/peerjs.min.js"></script>
    <script src="libs/tiny.toast-1.0.0/toast.min.js"></script>
    <link href="libs/xaringanExtra-broadcast-0.2.6/broadcast.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-broadcast-0.2.6/broadcast.js"></script>
    <script src="libs/fabric-4.3.1/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble-0.0.1/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble-0.0.1/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#a4c4f4"],"pen_size":8,"eraser_size":30,"palette":[]}) })</script>
    <script src="libs/xaringanExtra_fit-screen-0.2.6/fit-screen.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="PytorchLecture.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Introduction to PyTorch
]
.author[
### Riley Harper
]
.institute[
### UNC-Chapel Hill
]

---



class: center, middle

# **What is &lt;img src="./media/pytorch_logo.png" alt = "Pytorch Logo" height="60px" style="vertical-align: middle; filter: contrast(1.3) brightness(0.9);"/&gt; ?**

### An open-source deep learning (and ML) library for Python, primarily developed by Facebook's AI Research (FAIR) team.

###### *with major contributions from Adam Paszke, Soumith Chintala, Sam Gross, Gregory Chanan, and Zeming Lin.

---
class: middle

# Why PyTorch?
- Easy to learn and use
- Dynamic computation graph (define-by-run)
- Strong community and industry adoption
- Extensive support for deep learning tasks

---
class: center, middle

### PyTorch vs Competitors

&lt;table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; &lt;span style="     visibility: hidden;"&gt;&lt;/span&gt; &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; PyTorch &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; TensorFlow &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; Keras &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; JAX &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; Dynamic computation graph &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Yes (define-by-run) &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; No (Static by default, eager execution available) &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; No (Runs on TensorFlow) &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Yes &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; Ease of use &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; High (Pythonic) &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; High &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; Deployment options &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; High &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; High &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; Community support &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Strong &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Strong &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Strong &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Growing &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
class: center, middle

### PyTorch vs Competitors (cont.)
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; &lt;span style="     visibility: hidden;"&gt;&lt;/span&gt; &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; PyTorch &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; TensorFlow &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; Keras &lt;/th&gt;
   &lt;th style="text-align:left;text-align: center;"&gt; JAX &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; Popular for research &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Very popular &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Popular &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Less popular &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Increasing &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; GPU support &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Excellent (via CUDA) &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Excellent &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Via TensorFlow &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Excellent &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;background-color: rgba(242, 242, 242, 255) !important;"&gt; Model interpretability &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Good &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
   &lt;td style="text-align:left;text-align: center;"&gt; Moderate &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
class: middle
# Tensors
### PyTorch's basic data structure, similar to NumPy arrays but with the following additional features,
- **GPU acceleration**, for faster computations.
- **Autograd support**, enabling automatic differentiation for neural network training.
- **Multi-dimensionality**, ranging from scalars to high-dimensional arrays for handling complex data like images and time series.

---
class: middle
# Tensors Example

``` python
import torch
x = torch.tensor([1.0, 2.0, 3.0])
print(x)
```

```
## tensor([1., 2., 3.])
```

---
class: middle
# &lt;div style="font-size: 0.8em"&gt;Autograd (Automatic Differentiation) &lt;/div&gt;
### PyTorch's mechanism for automatic differentiation, useful for:
- **Tracking gradients**: Allows tensors to remember operations for backpropagation.
- **Enabling gradient-based optimization**: Useful for training neural networks by adjusting weights using gradients.
- **Ease of use**: Automatically calculates gradients without needing to manually compute derivatives.

---
class: middle
# Autograd Example

``` python
import torch

# Creating a tensor with requires_grad=True to track gradients
x = torch.tensor(3.0, requires_grad=True)
y = x**2
y.backward()  # Perform backpropagation
print(x.grad)  # Print the gradient of y with respect to x
```

---
class: middle
### **Backpropagation**

- Fundamental algorithm in deep learning used to train neural networks. 
- Computes the gradient of the loss function with respect to each weight in the network, allowing the model to learn by updating these weights using gradient descent.
- Applies the chain rule to compute gradients layer by layer, enabling efficient gradient calculation for large networks
- PyTorchâ€™s **Autograd** simplifies this by automatically tracking operations on tensors and computing gradients, making backpropagation more manageable for complex models.

---
class: middle

### **Backpropagation Example**

#### Given the function `\(y = x^2\)`, let `\(x = 3.0\)` and compute the gradient of `\(y\)` with respect to `\(x\)` using PyTorch's **Autograd**.

#### Backpropagation steps:
&lt;ol&gt;
&lt;li style="font-size: 1em;"&gt;&lt;b&gt;Define the function&lt;/b&gt;: \( y = x^2 \)&lt;/li&gt;
&lt;li style="font-size: 1em;"&gt;&lt;b&gt;Compute the gradient&lt;/b&gt;: The derivative of \( y \) with respect to \( x \) is \( \frac{dy}{dx} = 2x \)&lt;/li&gt;
&lt;li style="font-size: 1em;"&gt;&lt;b&gt;Evaluate the gradient at \( x = 3.0 \)&lt;/b&gt;: \( \frac{dy}{dx} = 2(3.0) = 6.0 \)&lt;/li&gt;
&lt;li style="font-size: 1em;"&gt;&lt;b&gt;PyTorch computes this gradient automatically&lt;/b&gt; and stores it in  \( x.grad \).&lt;/li&gt;
&lt;/ol&gt;

#### Result:
&lt;ul&gt;
&lt;li style="font-size: 1em;"&gt;The computed gradient is: \( \frac{dy}{dx} = 6.0 \)&lt;/li&gt;
&lt;/ul&gt;

---
class: middle
# Autograd Example w Result

``` python
import torch

# Creating a tensor with requires_grad=True to track gradients
x = torch.tensor(3.0, requires_grad=True)
y = x**2
y.backward()  # Perform backpropagation
print(x.grad)  # Print the gradient of y with respect to x
```

```
## tensor(6.)
```

---
class: middle
&lt;style&gt;
.ANN-text {
  font-weight: normal;
}
&lt;/style&gt;

#### &lt;b&gt;Artificial Neural Networks&lt;/b&gt;

##### &lt;span class="ANN-text"&gt;Artificial Neural Networks (ANNs) are computational models inspired by the human brain, consisting of layers of interconnected neurons. Each neuron receives input, processes it using a weighted sum and an activation function, and passes the result to the next layer. ANNs are widely used in deep learning for tasks like image recognition, natural language processing, and more [&lt;a href="https://link.springer.com/article/10.1007/BF02478259" target="_blank"&gt;McCulloch &amp; Pitts, 1943&lt;/a&gt;][&lt;a href="https://psycnet.apa.org/doi/10.1037/h0042519" target="_blank"&gt;Rosenblatt, 1958&lt;/a&gt;][&lt;a href="https://www.nature.com/articles/323533a0" target="_blank"&gt;Rumelhart, Hinton, &amp; Williams, 1986&lt;/a&gt;][&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" target="_blank"&gt;LeCun et al., 1998&lt;/a&gt;].&lt;/span&gt;

&lt;ul&gt;
&lt;li style="font-size: 0.8em;"&gt;&lt;b&gt;Input Layer&lt;/b&gt;: Receives the input data. 
    &lt;br&gt; 
    &lt;i&gt;Example&lt;/i&gt;: For fMRI image analysis, the input layer might take voxel intensity values from a 3D fMRI scan, where each voxel represents brain activity at a given point in space.&lt;/li&gt;

&lt;li style="font-size: 0.8em;"&gt;&lt;b&gt;Hidden Layers&lt;/b&gt;: Perform computations and transformations on the data. 
    &lt;br&gt; 
    &lt;i&gt;Example&lt;/i&gt;: In fMRI data analysis, hidden layers could detect patterns or regions of interest related to brain activity, such as identifying active regions during a specific cognitive task.&lt;/li&gt;

&lt;li style="font-size: 0.8em;"&gt;&lt;b&gt;Output Layer&lt;/b&gt;: Produces the final result based on the previous layers. 
    &lt;br&gt; 
    &lt;i&gt;Example&lt;/i&gt;: The output layer in an fMRI model might classify brain activity as corresponding to different cognitive states, such as distinguishing between a resting state and task-induced activity.&lt;/li&gt;

&lt;li style="font-size: 0.8em;"&gt;&lt;b&gt;Weights and Biases&lt;/b&gt;: Parameters that the model learns to minimize the error. 
    &lt;br&gt; 
    &lt;i&gt;Example&lt;/i&gt;: During training, the model adjusts the weights and biases to correctly predict patterns in brain activity, such as recognizing specific neural signatures linked to diseases like Alzheimer's.&lt;/li&gt;

&lt;li style="font-size: 0.8em;"&gt;&lt;b&gt;Activation Function&lt;/b&gt;: Introduces non-linearity to allow learning complex patterns. 
    &lt;br&gt; 
    &lt;i&gt;Example&lt;/i&gt;: The ReLU (Rectified Linear Unit) activation function could be used in an fMRI model to detect complex non-linear relationships between brain regions, ensuring the model captures intricate patterns of brain activity.&lt;/li&gt;
&lt;/ul&gt;

---
class: middle
# Building a Neural Network


``` python
import torch.nn as nn

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.fc(x)

model = Net()
print(model)
```

```
## Net(
##   (fc): Linear(in_features=10, out_features=1, bias=True)
## )
```

---
class: middle
# Training Loop


``` python
import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Dummy data
inputs = torch.randn(10)
targets = torch.randn(1)

# Training step
optimizer.zero_grad()
output = model(inputs)
loss = criterion(output, targets)
loss.backward()
optimizer.step()

print('Loss:', loss.item())
```

```
## Loss: 0.7900584936141968
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)


``` python
import torch
import torch.nn as nn
import nilearn
from nilearn.datasets import fetch_neurovault_motor_task
from nilearn.input_data import NiftiMasker

# Load fMRI data from the NeuroVault motor task dataset
# NeuroVault is a repository for brain imaging data, and the motor task dataset contains fMRI images from participants performing motor tasks
dataset = fetch_neurovault_motor_task()

# The dataset contains multiple fMRI images, we are selecting the first image from the dataset
fmri_img = dataset.images[0]

# Masking: Transform the 3D fMRI image into a 2D array where each voxel is treated as a feature
# NiftiMasker standardizes the data (voxel intensities) and prepares it for machine learning
# This will flatten the 3D image into a 2D array with shape (number of scans, number of voxels)
masker = NiftiMasker(standardize=True)
fmri_data = masker.fit_transform(fmri_img)
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)


``` python
# Define a simple neural network for fMRI data analysis
class fMRI_Net(nn.Module):
    def __init__(self):
        super(fMRI_Net, self).__init__()
        # The first fully connected layer takes the number of voxels as input (e.g., 1000+ voxels)
        # and outputs 128 features. The number of input features is dynamically set by fmri_data.shape[1]
        self.fc1 = nn.Linear(fmri_data.shape[1], 128)  # Input to hidden layer
        
        # The second fully connected layer reduces 128 features to 64 features
        self.fc2 = nn.Linear(128, 64)  # Hidden layer
        
        # The output layer has 2 neurons for binary classification (e.g., motor task vs. rest)
        self.fc3 = nn.Linear(64, 2)  # Output layer
        
        # ReLU activation function for non-linearity, used after the hidden layers
        self.relu = nn.ReLU()
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)


``` python
    # Define the forward pass of the network
    def forward(self, x):
        x = self.relu(self.fc1(x))  # Apply first layer and ReLU activation
        x = self.relu(self.fc2(x))  # Apply second layer and ReLU activation
        x = self.fc3(x)  # Output layer (no activation function needed for the output)
        return x

# Create a PyTorch tensor from the first scan's voxel data (fmri_data[0])
# This converts the fMRI data into a tensor of floats to be used in the network
fMRI_input = torch.tensor(fmri_data[0], dtype=torch.float32)

# Initialize the neural network
model = fMRI_Net()

# Perform a forward pass through the network with the fMRI input data
output = model(fMRI_input)
```

---
class: middle
# Neural Network Example for fMRI Data (with OpenNeuro)




``` python
# Print the architecture of the model (layers and their sizes)
print(model)
```

```
## fMRI_Net(
##   (fc1): Linear(in_features=45448, out_features=128, bias=True)
##   (fc2): Linear(in_features=128, out_features=64, bias=True)
##   (fc3): Linear(in_features=64, out_features=2, bias=True)
##   (relu): ReLU()
## )
```

``` python
# Print the output of the model
# This will be a tensor with 2 values representing the output from the 2 neurons in the final layer
# These 2 values can be interpreted as the scores for the 2 classes (e.g., motor task vs. rest)
print(output)
```

```
## tensor([-0.1668,  0.2590], grad_fn=&lt;ViewBackward0&gt;)
```
---
class: center, middle
# Questions?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
